{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "part1",
   "metadata": {},
   "source": [
    "# Part 1) Choice of Data Source\n",
    "\n",
    "For this analysis, we utilize two datasets:\n",
    "- **Fast Food Restaurants Across America** (Kaggle): A list of 10,000 fast food restaurants, including each restaurant's name, address, city, state, and geographic coordinates.\n",
    "- **Dangerous Places in USA: 2013 - 2018** (Kaggle): A record of incidents of gun violence in the USA from 2013 to 2018, including location (latitude/longitude) and incident details (number of people injured/killed, etc).\n",
    "\n",
    "**Data Format and Structure:** Both datasets are provided as CSV files. The fast food dataset is split across two CSV files which we will merge. The gun violence dataset is a single large CSV file.\n",
    "\n",
    "**Potential Pitfalls and Quirks:**\n",
    "- The fast food dataset is split into two files with overlapping entries. We need to merge them and remove duplicates.\n",
    "- State names in the fast food data are given as two-letter abbreviations; we'll convert these to full state names for clarity.\n",
    "- Date fields are in string format (ISO timestamp) and need parsing to datetime.\n",
    "- Some entries have missing data, e.g., missing coordinates. We will drop those records since location is critical for our analysis.\n",
    "- In the gun violence data, multiple gun types or statuses can be listed in one field as combined strings (separated by '||' or in '::' format). This requires parsing if we want to analyze gun types.\n",
    "- The gun violence dataset is large, so performance and memory need to be considered when processing (e.g., writing out chunks to Excel, using efficient merges)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2",
   "metadata": {},
   "source": [
    "## Part 2) Choice of Frameworks\n",
    "\n",
    "Investigate data processing and visualization frameworks and choose which ones to use in your project.\n",
    "Create a brief comparison of the frameworks you have researched, their benefits and drawbacks and how they can interact / support each other.\n",
    "\n",
    "Frameworks used:\n",
    "`numpy`, `pandas`, `matplotlib`, `datetime`, `scipy`, `geopandas`, `shapely`, `openpyxl`, `contextily`, `kagglehub`, `threading`, `concurrent.futures`, `multiprocessing`\n",
    "\n",
    "| Framework                                 | Purpose / Use Case                                                          | Benefits                                                                  | Drawbacks                                                             | Interactions/Support                       |\n",
    "|-------------------------------------------|----------------------------------------------------------------------------|---------------------------------------------------------------------------|-----------------------------------------------------------------------|--------------------------------------------|\n",
    "| **pandas**                                | Data handling, filtering, merging, cleaning, statistics, exporting to Excel | Rich in features, widely used, integrates well with plotting and file I/O | Can be memory-heavy for large datasets                                | Works with NumPy, Matplotlib, OpenPyXL     |\n",
    "| **numpy**                                 | Efficient numerical computation and transformation (e.g. square roots)      | Very fast with large numeric arrays and broadcasting                      | More complex syntax for beginners                                     | Backbone for Pandas                        |\n",
    "| **matplotlib**                            | Static plotting (bar plots, pie charts, etc.)                               | High customizability, widely supported                                    | Verbose syntax, complex layout tuning (e.g. `tight_layout` warnings)  | Works with Pandas, NumPy                   |\n",
    "| **datetime**                              | Timestamp parsing and formatting                                            | Native to Python, integrates with pandas                                  | Not vectorized like Pandas datetime functions                         | Converts formats for Excel export          |\n",
    "| **geopandas**                             | Geographic processing and spatial joins using shapefiles/coordinates        | Extends pandas for geospatial workflows                                   | Depends on installation of compiled packages (Shapely, Fiona, pyproj) | Integrates with Matplotlib and Shapely     |\n",
    "| **shapely**                               | Geometry operations like distance calculation between lat/lon points        | Simple geometric API                                                      | Requires EPSG projection setup for accurate distance                  | Used in GeoPandas                          |\n",
    "| **openpyxl**                              | Export to Excel formats including `.xlsx`                                   | Full control over Excel output                                            | Slower than CSV for large files                                       | Used by Pandas Excel backend               |\n",
    "| **contextily**                            | Adds tiled web maps to geospatial plots                                     | Allows beautiful basemap integration (e.g. OpenStreetMap)                 | Requires good CRS and internet connection                              | Works with GeoPandas plots                 |\n",
    "| **kagglehub**                             | Programmatic dataset download from Kaggle                                   | Automates reproducibility and avoids manual download                      | Requires Kaggle token/API setup                                       | Downloads into usable directory for Pandas |\n",
    "| **threading / futures / multiprocessing** | Accelerated spatial queries across restaurants                              | Parallelism across CPU cores for faster execution                         | Requires careful locking / data structure coordination                | Handles each restaurant individually       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3",
   "metadata": {},
   "source": [
    "## Part 3) Loading the Data\n",
    "\n",
    "In this section, we acquire the data from the online sources and load it into our Python environment.\n",
    "\n",
    "We use Kaggle's API via the `kagglehub` library to download the latest version of each dataset. After downloading, we identify the relevant CSV files for each dataset and prepare to load them using pandas.\n",
    "\n",
    "**Loading Steps:**\n",
    "1. **Download datasets** – Using `kagglehub.dataset_download()`, we retrieve the fast food restaurant dataset and the gun violence dataset.\n",
    "2. **Locate files** – We use Python's `glob` to find the CSV files in the downloaded directories.\n",
    "3. **Read data into pandas** – We will read the CSV files into pandas DataFrames for further processing (this is done in the cleaning step)."
   ]
  },
  {
   "cell_type": "code",
   "id": "part3-code",
   "metadata": {},
   "source": [
    "# Install dependencies as needed:\n",
    "# pip install kagglehub[pandas-datasets]\n",
    "import threading\n",
    "import geopandas\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import os.path\n",
    "import openpyxl\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.colors as mathcolors\n",
    "import contextily as ctx\n",
    "\n",
    "from tqdm import tqdm\n",
    "from threading import Lock\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "\n",
    "print(\"-------------------Download Datasets-------------------\")\n",
    "# Download the latest version of the fast food restaurants dataset\n",
    "path = kagglehub.dataset_download(\"imtkaggleteam/fast-food-restaurants-across-america\")\n",
    "print(\"Path to dataset files for fast food restaurants:\", path)\n",
    "\n",
    "# Download the latest version of the gun violence dataset\n",
    "path2 = kagglehub.dataset_download(\"jameslko/gun-violence-data\")  # \"sobhanmoosavi/us-accidents\"\n",
    "print(\"Path to dataset files for gun violence:\", path2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "part4",
   "metadata": {},
   "source": [
    "## Part 4) Cleaning the data\n",
    "\n",
    "Now that the raw data is loaded, we perform cleaning and preprocessing to ensure it is consistent and ready for analysis.\n",
    "\n",
    "**Cleaning Steps:**\n",
    "- **Merge and Deduplicate:** The fast food data comes in two files (possibly from different dates). We concatenate them and remove duplicate entries (identified by common fields like id, address, etc.) to get a unified list of restaurants.\n",
    "- **Handle Missing Fields:** In the older fast food file, the `primaryCategories` field might be missing. We add this column if necessary to merge the datasets properly.\n",
    "- **Standardize State Names:** We convert state abbreviations (e.g., 'CA') to full names (e.g., 'California') for clarity.\n",
    "- **Normalize Dates:** Convert `dateAdded` and `dateUpdated` from string timestamps to a standard datetime format.\n",
    "- **Normalize Text:** Clean the restaurant names (strip whitespace, standardize casing) to avoid inconsistencies (e.g., 'McDonald\\'s' vs 'mcdonald\\'s').\n",
    "- **Remove Invalid Entries:** Drop any entries that lack crucial information (such as latitude or longitude) since we cannot map those.\n",
    "- **Load Gun Violence Data:** Load the gun violence incidents CSV and similarly drop incidents with missing coordinates.\n",
    "- **Prepare for Integration:** Ensure both datasets are in usable form. We write out the cleaned fast food data to a CSV (and Excel) for reference. We also prepare to combine the data sets by spatial location (this will be done in the exploration step by finding incidents within a certain distance of each restaurant)."
   ]
  },
  {
   "cell_type": "code",
   "id": "part4-code-1",
   "metadata": {},
   "source": [
    "print(\"-------------------Clean Datasets-------------------\")\n",
    "# Find all CSV files in the fast food dataset directory\n",
    "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "# Read the two CSV files for fast food restaurants\n",
    "df1 = pd.read_csv(path + \"\\\\Datafiniti_Fast_Food_Restaurants.csv\")\n",
    "df2 = pd.read_csv(path + \"\\\\Datafiniti_Fast_Food_Restaurants_Jun19.csv\")\n",
    "\n",
    "# Ensure 'primaryCategories' exists in df1 (if missing)\n",
    "if 'primaryCategories' not in df1.columns:\n",
    "    df1['primaryCategories'] = \"\"\n",
    "\n",
    "# Align columns of df1 to match df2\n",
    "df1 = df1[df2.columns]\n",
    "\n",
    "# Identify new rows in df1 that are not in df2\n",
    "compare_cols = [col for col in df2.columns if col != 'primaryCategories']\n",
    "new_rows = df1.merge(df2, on=compare_cols, how='right', indicator=True)\n",
    "new_unique = new_rows[new_rows['_merge'] == 'right_only']\n",
    "\n",
    "# Concatenate df2 with the new unique rows from df1\n",
    "merged_df = pd.concat([df2, new_unique], ignore_index=True)\n",
    "\n",
    "# Create unified 'primaryCategories' column (use y from df1 where available)\n",
    "def fill_primary_categories(row):\n",
    "    if pd.isna(row['primaryCategories']) or str(row['primaryCategories']).strip() == '':\n",
    "        if row['_merge'] == 'right_only':\n",
    "            return row['primaryCategories_y']\n",
    "        else:\n",
    "            return row['primaryCategories_x']\n",
    "    else:\n",
    "        return row['primaryCategories']\n",
    "\n",
    "merged_df['primaryCategories'] = merged_df.apply(fill_primary_categories, axis=1)\n",
    "merged_df.drop(columns=['primaryCategories_x', 'primaryCategories_y', '_merge'], inplace=True)\n",
    "\n",
    "# Remove any remaining duplicate entries across both files\n",
    "df_unique = merged_df.drop_duplicates(\n",
    "    subset=[\n",
    "        'id', 'address', 'categories', 'city', 'country',\n",
    "        'latitude', 'longitude', 'name', 'postalCode',\n",
    "        'country', 'province', 'websites', 'primaryCategories', 'sourceURLs'\n",
    "    ],\n",
    "    keep='first'\n",
    ").copy()\n",
    "\n",
    "# Check how many entries remain after merging\n",
    "print(\"Total restaurants after merging and deduplication:\", len(df_unique))\n",
    "print(df_unique.head(3))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "part4-code-2",
   "metadata": {},
   "source": [
    "# Replace state abbreviations with full names\n",
    "us_states = {\n",
    "    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas',\n",
    "    'CA': 'California', 'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware',\n",
    "    'FL': 'Florida', 'GA': 'Georgia', 'HI': 'Hawaii', 'ID': 'Idaho',\n",
    "    'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa', 'KS': 'Kansas',\n",
    "    'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',\n",
    "    'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi',\n",
    "    'MO': 'Missouri', 'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada',\n",
    "    'NH': 'New Hampshire', 'NJ': 'New Jersey', 'NM': 'New Mexico', 'NY': 'New York',\n",
    "    'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio', 'OK': 'Oklahoma',\n",
    "    'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina',\n",
    "    'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah',\n",
    "    'VT': 'Vermont', 'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia',\n",
    "    'WI': 'Wisconsin', 'WY': 'Wyoming', 'DC': 'District of Columbia'\n",
    "}\n",
    "df_unique.loc[:, 'province'] = df_unique['province'].map(us_states).fillna(df_unique['province'])\n",
    "\n",
    "# Parse date strings to datetime and format\n",
    "df_unique['dateAdded'] = pd.to_datetime(df_unique['dateAdded'], errors='coerce')\n",
    "df_unique['dateUpdated'] = pd.to_datetime(df_unique['dateUpdated'], errors='coerce')\n",
    "\n",
    "df_unique['dateAdded'] = df_unique['dateAdded'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S') if pd.notnull(x) else \"\")\n",
    "df_unique['dateUpdated'] = df_unique['dateUpdated'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S') if pd.notnull(x) else \"\")\n",
    "\n",
    "# Normalize restaurant names (capitalize and strip whitespace)\n",
    "def normalize_name(name):\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    name = str(name).strip().lower()\n",
    "    return name.capitalize()\n",
    "\n",
    "df_unique.loc[:, 'name'] = df_unique['name'].astype(str).map(normalize_name)\n",
    "\n",
    "# Save the cleaned fast food data to CSV\n",
    "df_unique.to_csv(path + \"\\\\fast_food_vereint_ohne_duplikate.csv\", index=False)\n",
    "\n",
    "# Quick example: list some restaurants in California after cleaning\n",
    "california_restaurants = merged_df[merged_df[\"province\"] == \"CA\"]\n",
    "print(california_restaurants[['name','city','province','categories']].head(3))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "part4-code-3",
   "metadata": {},
   "source": [
    "# Reload the cleaned fast food data (for analysis) and also save to Excel\n",
    "df_fast = pd.read_csv(path + \"\\\\fast_food_vereint_ohne_duplikate.csv\")\n",
    "df_fast.to_excel(path + \"\\\\fast_food_vereint_ohne_duplikate.xlsx\", index=False)\n",
    "\n",
    "print(\"Path to dataset files for gun violence:\", path2)\n",
    "# Find CSV files in the gun violence dataset directory\n",
    "csv_files2 = glob.glob(os.path.join(path2, \"*.csv\"))\n",
    "\n",
    "# Load the gun violence dataset (this file is large)\n",
    "df_gun = pd.read_csv(path2 + \"\\\\gun-violence-data_01-2013_03-2018.csv\")\n",
    "\n",
    "# Drop incidents or restaurants with missing coordinates (cannot plot those)\n",
    "df_fast = df_fast.dropna(subset=[\"latitude\", \"longitude\"])\n",
    "df_gun = df_gun.dropna(subset=[\"latitude\", \"longitude\"])\n",
    "\n",
    "# (Optional) Save the large gun dataset into multiple Excel parts for easier external inspection\n",
    "chunk_size = 1_000_000\n",
    "if not os.path.exists(path2 + \"\\\\gun-violence-data_01-2013_03-2018_Part1.xlsx\"):\n",
    "    for i in range(0, len(df_gun), chunk_size):\n",
    "        chunk = df_gun.iloc[i:i + chunk_size]\n",
    "        file_name = path2 + f\"\\\\gun-violence-data_01-2013_03-2018_Part{i//chunk_size + 1}.xlsx\"\n",
    "        chunk.to_excel(file_name, index=False)\n",
    "        print(f\"Saved chunk to {file_name}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "part5",
   "metadata": {},
   "source": [
    "## Part 5) Fundamental Exploration\n",
    "\n",
    "With clean data in hand, we now perform an exploratory data analysis on both datasets:\n",
    "\n",
    "- **Descriptive Statistics:** For each attribute in the fast food and gun violence data, we calculate summary statistics. Numerical attributes (e.g., latitude, longitude) have their count, min, max, mean, and standard deviation computed. Categorical attributes (e.g., restaurant category, incident factors) are tallied by frequency. Date fields are examined to find the time span covered and average frequency of records per day. Text fields (like URLs or description text) are analyzed by length.\n",
    "- **Geospatial Analysis:** We integrate the two datasets by spatial proximity. For each restaurant, we find all gun violence incidents within a 300 meter radius. This allows us to identify which restaurants had nearby incidents and how many.\n",
    "- **Identification of Extremes:** Using the results of the spatial join, we identify the restaurants with the highest number of incidents (and casualties) nearby. We then visualize the \"top 10 most dangerous\" restaurant locations using bar charts.\n",
    "- **Category Distributions:** We further explore specific aspects of the gun violence data, such as the proportion of incidents involving stolen guns and the distribution of types of firearms used. These are visualized with pie charts.\n",
    "- **Spatial Visualization:** Finally, we plot all restaurant locations on a map of the US, highlighting those with incidents. Restaurants with incidents are shown in colors (from green to red) indicating the number of incidents, while those with none are shown in blue, giving a visual overview of the data's geographic distribution."
   ]
  },
  {
   "cell_type": "code",
   "id": "part5-code-1",
   "metadata": {},
   "source": [
    "print(\"-------------------Fundamental Exploration of Datasets-------------------\")\n",
    "# List columns of each DataFrame\n",
    "print(\"Fast food data columns:\", df_fast.columns.tolist())\n",
    "print(\"Gun violence data columns:\", df_gun.columns.tolist())\n",
    "\n",
    "# Define helper functions for exploration\n",
    "def explore_numerical(df, col):\n",
    "    data = df[col].dropna()\n",
    "    return {\n",
    "        \"count\": len(data),\n",
    "        \"min\": data.min(),\n",
    "        \"max\": data.max(),\n",
    "        \"mean\": data.mean(),\n",
    "        \"std\": data.std(),\n",
    "        \"min_indices\": data[data == data.min()].index.tolist() or \"None found\",\n",
    "        \"max_indices\": data[data == data.max()].index.tolist() or \"None found\",\n",
    "        \"mean_indices\": data[np.isclose(data, data.mean())].index.tolist() or \"None found\"\n",
    "    }\n",
    "\n",
    "def explore_categorical(df, col, max_categories=20):\n",
    "    vc = df[col].value_counts(dropna=False)\n",
    "    top = vc.head(max_categories).to_dict()\n",
    "    if len(vc) > max_categories:\n",
    "        top[\"__other__\"] = vc.iloc[max_categories:].sum()\n",
    "    return top\n",
    "\n",
    "def explore_datetime(df, col):\n",
    "    try:\n",
    "        data = pd.to_datetime(df[col], errors=\"coerce\").dropna()\n",
    "    except Exception:\n",
    "        data = pd.to_datetime(df[col], errors=\"coerce\").dropna()\n",
    "    if data.empty:\n",
    "        return {\"valid_dates\": 0}\n",
    "    delta = (data.max() - data.min()).days\n",
    "    return {\n",
    "        \"start\": str(data.min()),\n",
    "        \"end\": str(data.max()),\n",
    "        \"days\": delta,\n",
    "        \"total_entries\": len(data),\n",
    "        \"avg_entries_per_day\": round(len(data)/delta, 2) if delta > 0 else np.nan\n",
    "    }\n",
    "\n",
    "def explore_text(df, col):\n",
    "    data = df[col].dropna().astype(str)\n",
    "    lengths = data.str.len()\n",
    "    avg_len = lengths.mean()\n",
    "    min_len = lengths.min()\n",
    "    max_len = lengths.max()\n",
    "    return {\n",
    "        \"count\": len(data),\n",
    "        \"avg_length\": avg_len,\n",
    "        \"max_length\": max_len,\n",
    "        \"min_length\": min_len,\n",
    "        \"max_indices\": lengths[lengths == max_len].index.tolist(),\n",
    "        \"min_indices\": lengths[lengths == min_len].index.tolist(),\n",
    "        \"mean_indices\": lengths[np.isclose(lengths, avg_len)].index.tolist(),\n",
    "        \"max_values\": data[lengths == max_len].tolist(),\n",
    "        \"min_values\": data[lengths == min_len].tolist(),\n",
    "        \"mean_values\": data[np.isclose(lengths, avg_len)].tolist()\n",
    "    }\n",
    "\n",
    "# Exclude certain identifier columns from analysis\n",
    "excluded_columns = {\n",
    "    'fastfood': ['id', 'keys'],\n",
    "    'gun_violence': ['incident_id']\n",
    "}\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(depth=3, sort_dicts=False, compact=True)\n",
    "\n",
    "# Analyze Fast Food dataset\n",
    "print(\"Fast Food Dataset Analysis:\")\n",
    "report = {'fastfood': {}}\n",
    "for col in df_fast.columns:\n",
    "    if col in excluded_columns['fastfood']:\n",
    "        continue\n",
    "    if df_fast[col].dtype in ['float64', 'int64']:\n",
    "        report['fastfood'][col] = explore_numerical(df_fast, col)\n",
    "    elif 'date' in col.lower():\n",
    "        report['fastfood'][col] = explore_datetime(df_fast, col)\n",
    "    elif df_fast[col].dtype == 'object':\n",
    "        if col in ['sourceURLs', 'websites']:\n",
    "            report['fastfood'][col] = explore_text(df_fast, col)\n",
    "        else:\n",
    "            report['fastfood'][col] = explore_categorical(df_fast, col, max_categories=55)\n",
    "pp.pprint(report)\n",
    "print(\"-------------------Analyse of Fastfood Dataset-------------------\")\n",
    "\n",
    "# Analyze Gun Violence dataset\n",
    "print(\"Gun Violence Dataset Analysis:\")\n",
    "report = {\"gun_violence\": {}}\n",
    "for col in df_gun.columns:\n",
    "    if col in [\"incident_id\"]:\n",
    "        continue\n",
    "    if df_gun[col].dtype in [\"float64\", \"int64\"]:\n",
    "        report[\"gun_violence\"][col] = explore_numerical(df_gun, col)\n",
    "    elif \"date\" in col.lower():\n",
    "        report[\"gun_violence\"][col] = explore_datetime(df_gun, col)\n",
    "    elif df_gun[col].dtype == \"object\":\n",
    "        report[\"gun_violence\"][col] = explore_categorical(df_gun, col, max_categories=55)\n",
    "pp.pprint(report)\n",
    "print(\"-------------------Analyse of Gun Violence Dataset-------------------\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "sub-geo",
   "metadata": {},
   "source": [
    "### Spatial Incident Matching\n",
    "Now we examine the spatial relationship between fast food locations and gun violence incidents. We use GeoPandas to create geospatial data frames for each dataset and then find all incidents within 300 meters of each restaurant. Each matching set of incidents is saved to an Excel file for that restaurant, and we compile a summary of incident counts per restaurant."
   ]
  },
  {
   "cell_type": "code",
   "id": "part5-code-2",
   "metadata": {},
   "source": [
    "# Convert dataframes to GeoDataFrames with appropriate coordinate reference system (CRS)\n",
    "gdf_fast = geopandas.GeoDataFrame(\n",
    "    df_fast,\n",
    "    geometry=geopandas.points_from_xy(df_fast[\"longitude\"], df_fast[\"latitude\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ").to_crs(epsg=3857)\n",
    "\n",
    "gdf_gun = geopandas.GeoDataFrame(\n",
    "    df_gun,\n",
    "    geometry=geopandas.points_from_xy(df_gun[\"longitude\"], df_gun[\"latitude\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ").to_crs(epsg=3857)\n",
    "# Prepare output directory for incident report files\n",
    "output_dir = os.path.join(path2, \"accident_reports_at_restaurants\")\n",
    "if os.path.exists(output_dir):\n",
    "    for f in os.listdir(output_dir):\n",
    "        os.remove(os.path.join(output_dir, f))\n",
    "else:\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Function to find incidents near a single restaurant\n",
    "completed_count = [0]\n",
    "count_lock = threading.Lock()\n",
    "\n",
    "summary_data_with_incidents = []\n",
    "summary_data_without_incidents = []\n",
    "\n",
    "def process_restaurant(restaurant, count_lock, completed_count_list, total_count):\n",
    "    distance_threshold_m = 300\n",
    "    try:\n",
    "        name = str(restaurant[\"name\"]).replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "        state = str(restaurant.get(\"province\", \"Unknown\")).replace(\" \", \"_\")\n",
    "        plz = str(restaurant.get(\"postalCode\", \"Unknown\")).replace(\" \", \"_\")\n",
    "        city = str(restaurant.get(\"city\", \"Unknown\")).replace(\" \", \"_\")\n",
    "        street = str(restaurant.get(\"address\", \"Unknown\")).replace(\" \", \"_\")\n",
    "        filename_search = f\"{name}, at {state}_{plz}_{city}_{street}\"\n",
    "        print(f\"Searching for gun incident near {filename_search}...\")\n",
    "        x, y = restaurant.geometry.x, restaurant.geometry.y\n",
    "\n",
    "        # Filter candidate incidents roughly within bounding box around the restaurant (for efficiency)\n",
    "        candidates = gdf_gun[\n",
    "            (gdf_gun.geometry.x >= x - distance_threshold_m) & (gdf_gun.geometry.x <= x + distance_threshold_m) &\n",
    "            (gdf_gun.geometry.y >= y - distance_threshold_m) & (gdf_gun.geometry.y <= y + distance_threshold_m)\n",
    "        ]\n",
    "        nearby = candidates[candidates.distance(restaurant.geometry) <= distance_threshold_m].copy()\n",
    "\n",
    "        # Add distance column\n",
    "        nearby[\"incident distance\"] = nearby.geometry.distance(restaurant.geometry)\n",
    "        # Replace NaN with \"Unknown\" in gun_stolen\n",
    "        nearby[\"gun_stolen\"] = nearby[\"gun_stolen\"].fillna(\"Unknown\")\n",
    "        # Drop geometry column before saving (not needed in Excel)\n",
    "        if \"geometry\" in nearby.columns:\n",
    "            nearby.drop(columns=[\"geometry\"], inplace=True)\n",
    "\n",
    "        # Use restaurant's dateAdded as part of filename if available\n",
    "        first_date = restaurant.get(\"dateAdded\", None)\n",
    "        if pd.notna(first_date):\n",
    "            try:\n",
    "                date_str = pd.to_datetime(first_date).strftime(\"%Y%m%d\")\n",
    "            except Exception:\n",
    "                date_str = datetime.now().strftime(\"%Y%m%d\")\n",
    "        else:\n",
    "            date_str = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "        filename = f\"{date_str}, {name}, at {state}_{plz}_{city}_{street}.xlsx\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        nearby.to_excel(filepath, index=False)\n",
    "\n",
    "        with count_lock:\n",
    "            completed_count_list[0] += 1\n",
    "            print(f\"Created gun incident report for {filename} ({completed_count_list[0]} of {total_count})\")\n",
    "\n",
    "        return {\n",
    "            \"name\": restaurant[\"name\"],\n",
    "            \"address\": restaurant[\"address\"],\n",
    "            \"state\": state,\n",
    "            \"city\": city,\n",
    "            \"postalCode\": plz,\n",
    "            \"num_incidents\": len(nearby),\n",
    "            \"total_killed\": nearby[\"n_killed\"].sum(),\n",
    "            \"total_injured\": nearby[\"n_injured\"].sum(),\n",
    "            \"gun_stolen_counts\": nearby[\"gun_stolen\"].value_counts().to_dict(),\n",
    "            \"restaurant_latitude\": restaurant[\"latitude\"],\n",
    "            \"restaurant_longitude\": restaurant[\"longitude\"]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        with count_lock:\n",
    "            completed_count_list[0] += 1\n",
    "            print(f\"Error processing restaurant ({completed_count_list[0]} of {total_count}): {e}\")\n",
    "        return None\n",
    "\n",
    "# Run the spatial matching in parallel for all restaurants\n",
    "num_threads = multiprocessing.cpu_count()\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    futures = [\n",
    "        executor.submit(process_restaurant, row, count_lock, completed_count, len(gdf_fast))\n",
    "        for _, row in gdf_fast.iterrows()\n",
    "    ]\n",
    "    for future in as_completed(futures):\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            if result[\"num_incidents\"] > 0:\n",
    "                summary_data_with_incidents.append(result)\n",
    "            else:\n",
    "                summary_data_without_incidents.append(result)\n",
    "# Create a summary DataFrame of all restaurants that had at least one incident nearby or none at all\n",
    "summary_path_with_incidents = path2 + \"\\\\gun_violence_at_restaurant_results_with_incidents.xlsx\"\n",
    "summary_df_with_incidents = pd.DataFrame(summary_data_with_incidents)\n",
    "\n",
    "summary_df_without_incidents = pd.DataFrame(summary_data_without_incidents)\n",
    "summary_path_without_incidents = path2 + \"\\\\gun_violence_at_restaurant_results_without_incidents.xlsx\"\n",
    "summary_df_without_incidents.to_excel(summary_path_without_incidents, index=False)\n",
    "print(\"✔ gun_violence_at_restaurant_results_without_incidents.xlsx gespeichert.\")\n",
    "\n",
    "\n",
    "# Add a column counting stolen weapons (excluding 'Unknown')\n",
    "def count_stolen_weapons(gun_dict):\n",
    "    if not isinstance(gun_dict, dict):\n",
    "        return 0\n",
    "    return sum(v for k, v in gun_dict.items() if k.lower() not in [\"unknown\", \"\"])\n",
    "summary_df_with_incidents[\"num_stolen_weapons\"] = summary_df_with_incidents[\"gun_stolen_counts\"].apply(count_stolen_weapons)\n",
    "\n",
    "# Save the summary to an Excel file\n",
    "summary_df_with_incidents.to_excel(summary_path_with_incidents, index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "sub-top10",
   "metadata": {},
   "source": [
    "### Top 10 Most Dangerous Restaurants\n",
    "Using the summary data, we identify the top 10 restaurant locations with the most incidents nearby. We consider not only the number of incidents, but also the severity (people killed or injured). Below we show bar charts for the top 10 locations ranked by number of incidents, by number of fatalities, and by number of injuries."
   ]
  },
  {
   "cell_type": "code",
   "id": "part5-code-3",
   "metadata": {},
   "source": [
    "# Identify top 10 dangerous restaurants by number of incidents (and other criteria as tiebreakers)\n",
    "top10_num_incidents = summary_df_with_incidents.sort_values(\n",
    "    by=[\"num_incidents\", \"total_killed\", \"total_injured\", \"num_stolen_weapons\"],\n",
    "    ascending=[False, False, False, False]\n",
    ").head(10)\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(depth=3, sort_dicts=False)\n",
    "# Print top 10 summary (name, address, incidents, killed, injured)\n",
    "pp.pprint(top10_num_incidents[[\"name\", \"address\", \"num_incidents\", \"total_killed\", \"total_injured\"]].to_dict(orient=\"records\"))\n",
    "\n",
    "# Bar plot: Top 10 by number of incidents\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(top10_num_incidents[\"name\"] + \" @ \" + top10_num_incidents[\"city\"] + \" (\" + top10_num_incidents[\"state\"] + \")\", top10_num_incidents[\"num_incidents\"])\n",
    "plt.title(\"Top 10 Restaurants by Number of Nearby Incidents\")\n",
    "plt.ylabel(\"Number of incidents\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top 10 by number of people killed\n",
    "top10_killed = summary_df_with_incidents.sort_values(\n",
    "    by=[\"total_killed\", \"total_injured\", \"num_stolen_weapons\", \"num_incidents\"],\n",
    "    ascending=[False, False, False, False]\n",
    ").head(10)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(top10_killed[\"name\"] + \" @ \" + top10_killed[\"city\"] + \" (\" + top10_killed[\"state\"] + \")\", top10_killed[\"total_killed\"])\n",
    "plt.title(\"Top 10 Restaurants by Number of People Killed in Nearby Incidents\")\n",
    "plt.ylabel(\"Total killed\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top 10 by number of people injured\n",
    "top10_injured = summary_df_with_incidents.sort_values(\n",
    "    by=[\"total_injured\", \"num_stolen_weapons\", \"num_incidents\", \"total_killed\"],\n",
    "    ascending=[False, False, False, False]\n",
    ").head(10)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(top10_injured[\"name\"] + \" @ \" + top10_injured[\"city\"] + \" (\" + top10_injured[\"state\"] + \")\", top10_injured[\"total_injured\"])\n",
    "plt.title(\"Top 10 Restaurants by Number of People Injured in Nearby Incidents\")\n",
    "plt.ylabel(\"Total injured\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "sub-guns",
   "metadata": {},
   "source": [
    "### Gun Type and Status Distributions\n",
    "Next, we analyze the distribution of firearm types and the status of guns (whether they were stolen or not) in the gun violence incidents. We parse the combined gun type entries and then visualize the breakdown."
   ]
  },
  {
   "cell_type": "code",
   "id": "part5-code-4",
   "metadata": {},
   "source": [
    "# Prepare data for gun type and stolen status analysis\n",
    "df_gun_clean = df_gun.dropna(subset=[\"gun_type\", \"gun_stolen\"])\n",
    "\n",
    "# Helper functions to split combined entries\n",
    "def extract_entries(entry):\n",
    "    return [item.split(\"::\")[1].strip() if \"::\" in item else item.strip() for item in str(entry).split(\"||\")]\n",
    "\n",
    "def classify_gun_type(gun):\n",
    "    gun = gun.lower()\n",
    "    if \"handgun\" in gun or \"pistol\" in gun or \"revolver\" in gun:\n",
    "        return \"Handgun\"\n",
    "    elif \"rifle\" in gun or \"ak\" in gun or \"ar-\" in gun or \"7.62\" in gun or \"carbine\" in gun:\n",
    "        return \"Rifle\"\n",
    "    elif \"shotgun\" in gun:\n",
    "        return \"Shotgun\"\n",
    "    elif \"bb gun\" in gun or \"air\" in gun:\n",
    "        return \"Airgun\"\n",
    "    elif \"unknown\" in gun:\n",
    "        return \"Unknown\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "# Split out each gun entry for all incidents\n",
    "type_list, stolen_list, classified_list = [], [], []\n",
    "for _, row in df_gun_clean.iterrows():\n",
    "    types = extract_entries(row[\"gun_type\"])\n",
    "    stolen = extract_entries(row[\"gun_stolen\"])\n",
    "    for t, s in zip(types, stolen):\n",
    "        type_list.append(t)\n",
    "        stolen_list.append(s)\n",
    "        classified_list.append(classify_gun_type(t))\n",
    "\n",
    "# Create DataFrame of each gun involved\n",
    "df_comb = pd.DataFrame({\n",
    "    \"gun_type\": type_list,\n",
    "    \"gun_stolen\": stolen_list,\n",
    "    \"gun_class\": classified_list\n",
    "})\n",
    "# Save a crosstab of gun class vs stolen status to Excel for reference\n",
    "classification_table = pd.crosstab(df_comb[\"gun_class\"], df_comb[\"gun_stolen\"])\n",
    "classification_table.to_excel(\"gun_classification.xlsx\")\n",
    "print(\"✔ gun_classification.xlsx saved.\")\n",
    "\n",
    "# Pie chart of gun stolen status distribution\n",
    "plt.figure(figsize=(6, 6))\n",
    "df_comb[\"gun_stolen\"].value_counts().plot.pie(autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"Gun Stolen Status Distribution\")\n",
    "plt.ylabel(\"\")\n",
    "plt.show()\n",
    "\n",
    "# Pie chart of classified gun type distribution\n",
    "plt.figure(figsize=(6, 6))\n",
    "df_comb[\"gun_class\"].value_counts().plot.pie(autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"Classified Gun Type Distribution\")\n",
    "plt.ylabel(\"\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "sub-map",
   "metadata": {},
   "source": [
    "### Geographic Visualization of Incidents\n",
    "Finally, we visualize the location of fast food restaurants on a map of the United States. Restaurants with no incidents nearby are shown in blue, while those with incidents are shown in a color gradient from green (fewer incidents) to red (more incidents)."
   ]
  },
  {
   "cell_type": "code",
   "id": "part5-code-5",
   "metadata": {},
   "source": [
    "# Plot the US map with fast food locations and incident counts\n",
    "# Create GeoDataFrames for restaurants with incidents and without incidents\n",
    "\n",
    "def collapse_small_entries(series, threshold=0.002):\n",
    "    total = series.sum()\n",
    "    proportions = series / total\n",
    "    large = series[proportions >= threshold]\n",
    "    small = series[proportions < threshold]\n",
    "    if not small.empty:\n",
    "        large[\"Other\"] = small.sum()\n",
    "    return large\n",
    "\n",
    "# Piechart for gun_stolen\n",
    "stolen_all = []\n",
    "for entry in df_gun_clean[\"gun_stolen\"]:\n",
    "    stolen_all.extend(extract_entries(entry))\n",
    "\n",
    "df_stolen = pd.Series(stolen_all).value_counts()\n",
    "df_stolen = collapse_small_entries(df_stolen)\n",
    "\n",
    "plt.figure(figsize=(18, 18))\n",
    "plt.pie(df_stolen, labels=df_stolen.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"Verteilung des Waffenstatus (gun_stolen)\")\n",
    "plt.show()\n",
    "\n",
    "# Piechart for gun_type\n",
    "type_all = []\n",
    "for entry in df_gun_clean[\"gun_type\"]:\n",
    "    type_all.extend(extract_entries(entry))\n",
    "\n",
    "df_types = pd.Series(type_all).value_counts()\n",
    "df_types = collapse_small_entries(df_types)\n",
    "\n",
    "plt.figure(figsize=(18, 18))\n",
    "plt.pie(df_types, labels=df_types.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"Verteilung der Waffentypen (gun_type)\")\n",
    "plt.show()\n",
    "\n",
    "# GeoDataFrame mit Projektion\n",
    "# GeoDataFrame für Restaurants mit Vorfällen\n",
    "geometry_with_incidents = [Point(xy) for xy in zip(summary_df_with_incidents[\"restaurant_longitude\"], summary_df_with_incidents[\"restaurant_latitude\"])]\n",
    "# GeoDataFrame für Restaurants ohne Vorfälle\n",
    "geometry_without_incidents = [Point(xy) for xy in zip(summary_df_without_incidents[\"restaurant_longitude\"], summary_df_without_incidents[\"restaurant_latitude\"])]\n",
    "\n",
    "gdf_with_incidents = geopandas.GeoDataFrame(summary_df_with_incidents, geometry=geometry_with_incidents, crs=\"EPSG:4326\").to_crs(epsg=3857)\n",
    "gdf_without_incidents = geopandas.GeoDataFrame(summary_df_without_incidents, geometry=geometry_without_incidents, crs=\"EPSG:4326\").to_crs(epsg=3857)\n",
    "\n",
    "# Prepare color mapping based on incident counts (sqrt scale to spread values)\n",
    "incident_counts = gdf_with_incidents[\"num_incidents\"]\n",
    "max_incidents = incident_counts.max()\n",
    "transformed = np.sqrt(incident_counts)\n",
    "max_trans = np.sqrt(max_incidents) if max_incidents > 0 else 0\n",
    "cmap = mathcolors.LinearSegmentedColormap.from_list(\"incident_cmap\", [\"green\", \"yellow\", \"red\"])\n",
    "norm = mathcolors.Normalize(vmin=0, vmax=max_trans)\n",
    "colors = [cmap(norm(x)) for x in transformed]\n",
    "\n",
    "# Plot base\n",
    "fig, ax = plt.subplots(figsize=(20, 20), constrained_layout=True)\n",
    "gdf_without_incidents.plot(ax=ax, color=\"blue\", markersize=5, alpha=0.5, label=\"No incidents\")\n",
    "gdf_with_incidents.plot(ax=ax, color=colors, markersize=20, alpha=0.7, label=\"Has incidents\")\n",
    "ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "ax.set_title(\"Fast Food Restaurants and Nearby Gun Incidents (2013-2018)\")\n",
    "ax.axis(\"off\")\n",
    "# Create colorbar for incident count scale\n",
    "tick_values = np.linspace(0, max_incidents, num=6)\n",
    "tick_positions = np.sqrt(tick_values)\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = fig.colorbar(sm, ax=ax, fraction=0.03, pad=0.04, ticks=tick_positions)\n",
    "cbar.ax.set_yticklabels([f\"{int(val)}\" for val in tick_values])\n",
    "cbar.set_label(\"Number of Incidents\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 📝 Summary: Gun Violence Near Fast-Food Restaurants (English)\n",
    "\n",
    "This analysis investigated the correlation between gun violence incidents and their proximity to fast-food restaurants across the United States. The main results and insights are:\n",
    "\n",
    "- **Number of Incidents**: Certain restaurant chains (e.g., Subway, Jimmy John's) have high incident counts near specific locations. For example, one Subway location had over **60 incidents**, making it the most affected.\n",
    "- **Severity**: Some locations saw significant fatalities and injuries, while others had frequent but non-lethal incidents. For instance, a Panera Bread location recorded **4 deaths** and **6 injuries**.\n",
    "- **Stolen Weapons**: We categorized gun theft data. The pie chart showed most entries were either `\"Unknown\"` or `\"Stolen\"`. The rarity of `\"Legal\"` or `\"Not stolen\"` implies low traceability.\n",
    "- **Weapon Types**: Most frequent were `\"Handgun\"` and `\"Unknown\"`, while `\"Rifle\"`, `\"Shotgun\"` and military-style weapons like `\"AK-47\"` were less common. Low-frequency entries were grouped as `\"Other\"`.\n",
    "- **Geospatial Heatmap**: A map was generated showing restaurants using a red-yellow-green scale depending on the number of incidents nearby. Restaurants with no nearby incidents were marked in blue.\n",
    "- **Classification Matrix**: A cross-tabulation of `gun_type` × `gun_stolen` combinations revealed insights into how often stolen vs. non-stolen weapons of certain types are used.\n",
    "\n",
    "The approach leverages multithreading to associate all gun violence entries within 300 meters of a restaurant, making the process scalable. Separate reports and aggregated summaries were generated for both incident-heavy and incident-free locations.\n",
    "\n",
    "This dataset highlights the potential of spatial analysis for public safety research and shows which fast-food chains may need targeted interventions.\n"
   ],
   "id": "9a571e0b9aa65512"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
